{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Spatial Transcriptomics Deep Learning (STDL) Project Notebook**\n",
    "\n",
    "> The notebook contains main experiments and examples of how to use the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Phase 1: Pre-processing and technical preparations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: **Assign GPU device and allow CUDA debugging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create code to reimport module if i change it\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda debugging allowed\n"
     ]
    }
   ],
   "source": [
    "# the next 2 lines are to allow debugging with CUDA !\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  \n",
    "print(f'cuda debugging allowed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda device count: 1\n",
      "Using device: cuda\n",
      "device name: GeForce RTX 2080 Ti\n",
      "torch.cuda.device(0): <torch.cuda.device object at 0x7f31302e4a10>\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "CPU times: user 1.88 s, sys: 6.16 s, total: 8.04 s\n",
      "Wall time: 5.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import torch\n",
    "print(f'cuda device count: {torch.cuda.device_count()}')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(f'device name: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'torch.cuda.device(0): {torch.cuda.device(0)}')\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "# NOTE: important !!!!!!\n",
    "# clearing out the cache before beginning\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: **Import the Pre-Process module**\n",
    "\n",
    "> `loadAndPreProcess` module contains methods to load the data files as pytorch and pandas objects, methods to preprocess the given data, and methods to create custom datasets from the preprocessed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>TODO:</b> fill above line\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: path to project is: /home/roy.rubin/STDLproject/\n",
    "%autoreload 2\n",
    "import loadAndPreProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: **Load pytorch dataset objects from the image folder**\n",
    "\n",
    "> loading regular and augmented datasets created from the given image folder with transformations.\n",
    "\n",
    "> Note: `augmentedImageFolder` is a custom dataset of imageFolder objects with different transformations (see code).\n",
    "\n",
    "> Note: `im_hight_and_width_size` will define the size to which the images in the folder will be resized to. their original size 176, and so if the number will be bigger, the images will be automaticaly upsampled in the `resize` (not sure by what method) - which means images might be \"pixelized\" / lower quality. The problem is, size 176 doesnt work with all models, so i had to increase the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_hight_and_width_size = 176  # values: 176 (doesnt work with inception) / 224 (doesnt work with inception) / 299 (works with inception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- entered function load_dataset_from_pictures_folder -----\n",
      "\n",
      "----- finished function load_dataset_from_pictures_folder -----\n",
      "\n",
      "\n",
      "----- entered function load_dataset_from_pictures_folder -----\n",
      "\n",
      "----- finished function load_dataset_from_pictures_folder -----\n",
      "\n",
      "CPU times: user 263 ms, sys: 27.7 ms, total: 291 ms\n",
      "Wall time: 512 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "path_to_images_dir_patient1_train = \"/home/roy.rubin/STDLproject/spatialGeneExpressionData/patient1/images\"\n",
    "imageFolder_train = loadAndPreProcess.load_dataset_from_images_folder(path_to_images_dir_patient1_train, im_hight_and_width_size)\n",
    "augmentedImageFolder_train = loadAndPreProcess.load_augmented_imageFolder_DS_from_images_folder(path_to_images_dir_patient1_train, im_hight_and_width_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- entered function load_dataset_from_pictures_folder -----\n",
      "\n",
      "----- finished function load_dataset_from_pictures_folder -----\n",
      "\n",
      "CPU times: user 21.6 ms, sys: 6.21 ms, total: 27.8 ms\n",
      "Wall time: 58.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "path_to_images_dir_patient2_test = \"/home/roy.rubin/STDLproject/spatialGeneExpressionData/patient2/images\"\n",
    "imageFolder_test = loadAndPreProcess.load_dataset_from_images_folder(path_to_images_dir_patient2_test, im_hight_and_width_size)\n",
    "# augmentedImageFolder_test = loadAndPreProcess.load_augmented_imageFolder_DS_from_images_folder(path_to_images_dir_patient2_test, im_hight_and_width_size) # not needed for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4: **Load pandas dataframe objects from the given mtx/tsv/csv files**\n",
    "\n",
    "> `matrix_dataframe` represents the gene expression count values of each sample for each gene\n",
    "\n",
    "> `features_dataframe` contains the names of all the genes\n",
    "\n",
    "> `barcodes_dataframe` contains the names of all the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- entered function load_dataframes_from_mtx_and_tsv -----\n",
      "started reading features.tsv\n",
      "V  finished reading features.tsv\n",
      "started reading barcodes.tsv\n",
      "V  finished reading barcodes.tsv\n",
      "started reading matrix.mtx. this might take some time ...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "path_to_mtx_tsv_files_dir_patient1_train = \"/home/roy.rubin/STDLproject/spatialGeneExpressionData/patient1\"\n",
    "matrix_dataframe_train, features_dataframe_train , barcodes_dataframe_train = loadAndPreProcess.load_dataframes_from_mtx_and_tsv_new(path_to_mtx_tsv_files_dir_patient1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "path_to_mtx_tsv_files_dir_patient2_test = \"/home/roy.rubin/STDLproject/spatialGeneExpressionData/patient2\"\n",
    "matrix_dataframe_test, features_dataframe_test , barcodes_dataframe_test = loadAndPreProcess.load_dataframes_from_mtx_and_tsv_new(path_to_mtx_tsv_files_dir_patient2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5: **Remove samples from the matrix dataframe with no matching images in the image folder**\n",
    "\n",
    "> Note: indices are being reset after this action, so a mapping of old to new column indices is returned: `column_mapping`.\n",
    "\n",
    "> Note: the dataframe is also reordered according to the images order in the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "matrix_dataframe_train, column_mapping_train = loadAndPreProcess.cut_samples_with_no_matching_image_and_reorder_df(matrix_df=matrix_dataframe_train, \n",
    "                                                                                                                    image_folder_of_the_df=imageFolder_train, \n",
    "                                                                                                                    barcodes_df=barcodes_dataframe_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "matrix_dataframe_test, column_mapping_test = loadAndPreProcess.cut_samples_with_no_matching_image_and_reorder_df(matrix_df=matrix_dataframe_test, \n",
    "                                                                                                                  image_folder_of_the_df=imageFolder_test, \n",
    "                                                                                                                  barcodes_df=barcodes_dataframe_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6: **Remove less-informative genes**\n",
    "\n",
    "> we define *less-informative* genes as genes with less than K counts over all samples\n",
    "\n",
    "> `Base_value` is a parameter for the user's choice\n",
    "\n",
    "> Note: indices are being reset after this action, so a mapping of old to new column indices is returned: `row_mapping`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# begin by asserting that our dataframes have the same genes to begin with using the metadata of features_dataframe\n",
    "assert features_dataframe_train['gene_names'].equals(features_dataframe_test['gene_names'])\n",
    "\n",
    "Base_value = 10\n",
    "matrix_dataframe_train, matrix_dataframe_test, row_mapping = loadAndPreProcess.cut_genes_with_under_B_counts_from_train_and_test(matrix_dataframe_train, matrix_dataframe_test, Base_value) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7: **Normalize matrix_dataframe entries**\n",
    "\n",
    "> normaliztion will be performed on the remainning rows of the dataframe with the logic \"log 1P\"\n",
    "\n",
    "> This method Calculates log(1 + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "matrix_dataframe_train = loadAndPreProcess.perform_log_1p_normalization(matrix_dataframe_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "matrix_dataframe_test = loadAndPreProcess.perform_log_1p_normalization(matrix_dataframe_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We have performed all of the pre-processing actions on our matrix dataframes. (more pre-processing is still needed our datasets)\n",
    "\n",
    "> print some information regarding our dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import projectUtilities\n",
    "projectUtilities.printInfoAboutReducedDF(matrix_dataframe_train)\n",
    "print(\"\\n****\\n\")\n",
    "projectUtilities.printInfoAboutReducedDF(matrix_dataframe_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8: **Create custom datasets**\n",
    "\n",
    "> Each custom dataset is tailored per task\n",
    "\n",
    "> there are four tasks: single gene prediction, k gene prediction, all gene prediction using NMF dim. reduction, all gene prediction using AE dim. reduction\n",
    "\n",
    "> For each of the above tasks 2 datasets were created:\n",
    "\n",
    ">> A Dataset created from the TRAIN data WITHOUT augmentation (without image transformations)\n",
    "\n",
    ">> A Dataset created from the TRAIN data WITH augmentation (with image transformations)\n",
    "\n",
    ">> A Dataset created from the TEST data WITHOUT augmentation (without image transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gene_name = 'MKI67'\n",
    "custom_DS_SingleValuePerImg = loadAndPreProcess.STDL_Dataset_SingleValuePerImg(imageFolder=imageFolder_train, \n",
    "                                                               matrix_dataframe=matrix_dataframe_train, \n",
    "                                                               features_dataframe=features_dataframe_train, \n",
    "                                                               barcodes_dataframe=barcodes_dataframe_train, \n",
    "                                                               column_mapping=column_mapping_train,\n",
    "                                                               row_mapping=row_mapping,\n",
    "                                                               chosen_gene_name=gene_name)\n",
    "custom_DS_SingleValuePerImg_augmented = loadAndPreProcess.STDL_Dataset_SingleValuePerImg(imageFolder=augmentedImageFolder_train, \n",
    "                                                               matrix_dataframe=matrix_dataframe_train, \n",
    "                                                               features_dataframe=features_dataframe_train, \n",
    "                                                               barcodes_dataframe=barcodes_dataframe_train, \n",
    "                                                               column_mapping=column_mapping_train,\n",
    "                                                               row_mapping=row_mapping,\n",
    "                                                               chosen_gene_name=gene_name)\n",
    "custom_DS_SingleValuePerImg_test = loadAndPreProcess.STDL_Dataset_SingleValuePerImg(imageFolder=imageFolder_test, \n",
    "                                                               matrix_dataframe=matrix_dataframe_test, \n",
    "                                                               features_dataframe=features_dataframe_test, \n",
    "                                                               barcodes_dataframe=barcodes_dataframe_test, \n",
    "                                                               column_mapping=column_mapping_test,\n",
    "                                                               row_mapping=row_mapping,\n",
    "                                                               chosen_gene_name=gene_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> inside the init phase of `STDL_Dataset_KValuesPerImg_KGenesWithHighestVariance` class, K genes with the highest variance are chosen from matrix_dataframe, and they are the only genes that are kept for training and testing purposes\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "k = 10\n",
    "custom_DS_KGenesWithHighestVariance = loadAndPreProcess.STDL_Dataset_KValuesPerImg_KGenesWithHighestVariance(imageFolder=imageFolder_train, \n",
    "                                                                           matrix_dataframe=matrix_dataframe_train, \n",
    "                                                                           features_dataframe=features_dataframe_train, \n",
    "                                                                           barcodes_dataframe=barcodes_dataframe_train, \n",
    "                                                                           column_mapping=column_mapping_train,\n",
    "                                                                           num_of_dims_k=k)\n",
    "custom_DS_KGenesWithHighestVariance_augmented = loadAndPreProcess.STDL_Dataset_KValuesPerImg_KGenesWithHighestVariance(imageFolder=augmentedImageFolder_train, \n",
    "                                                                           matrix_dataframe=matrix_dataframe_train, \n",
    "                                                                           features_dataframe=features_dataframe_train, \n",
    "                                                                           barcodes_dataframe=barcodes_dataframe_train, \n",
    "                                                                           column_mapping=column_mapping_train,\n",
    "                                                                           num_of_dims_k=k)\n",
    "custom_DS_KGenesWithHighestVariance_test = loadAndPreProcess.STDL_Dataset_KValuesPerImg_KGenesWithHighestVariance(imageFolder=imageFolder_test, \n",
    "                                                                           matrix_dataframe=matrix_dataframe_test, \n",
    "                                                                           features_dataframe=features_dataframe_test, \n",
    "                                                                           barcodes_dataframe=barcodes_dataframe_test, \n",
    "                                                                           column_mapping=column_mapping_test,\n",
    "                                                                           num_of_dims_k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> inside the init phase of `STDL_Dataset_KValuesPerImg_LatentTensor_NMF` class, an NMF decompositionis performed on the matrix_dataframe object\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "k = 10\n",
    "custom_DS_LatentTensor_NMF = loadAndPreProcess.STDL_Dataset_KValuesPerImg_LatentTensor_NMF(imageFolder=imageFolder_train, \n",
    "                                                                           matrix_dataframe=matrix_dataframe_train, \n",
    "                                                                           features_dataframe=features_dataframe_train, \n",
    "                                                                           barcodes_dataframe=barcodes_dataframe_train, \n",
    "                                                                           column_mapping=column_mapping_train,\n",
    "                                                                           num_of_dims_k=k)\n",
    "custom_DS_LatentTensor_NMF_augmented = loadAndPreProcess.STDL_Dataset_KValuesPerImg_LatentTensor_NMF(imageFolder=augmentedImageFolder_train, \n",
    "                                                                           matrix_dataframe=matrix_dataframe_train, \n",
    "                                                                           features_dataframe=features_dataframe_train, \n",
    "                                                                           barcodes_dataframe=barcodes_dataframe_train, \n",
    "                                                                           column_mapping=column_mapping_train,\n",
    "                                                                           num_of_dims_k=k)\n",
    "custom_DS_LatentTensor_NMF_test = loadAndPreProcess.STDL_Dataset_KValuesPerImg_LatentTensor_NMF(imageFolder=imageFolder_test, \n",
    "                                                                           matrix_dataframe=matrix_dataframe_test, \n",
    "                                                                           features_dataframe=features_dataframe_test, \n",
    "                                                                           barcodes_dataframe=barcodes_dataframe_test, \n",
    "                                                                           column_mapping=column_mapping_test,\n",
    "                                                                           num_of_dims_k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "<ul>\n",
    "  <li>first we create a dataset from `matrix_dataframe_train` to feed our AEnet.</li>\n",
    "  <li>Then we create our AEnet and train it.</li>\n",
    "  <li>Finally, we create our `custom_DS_LatentTensor_AE` class, in which the Autoencoder network will be saved.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_from_matrix_df = loadAndPreProcess.STDL_Dataset_matrix_df_for_AE_init(matrix_dataframe_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from executionModule import get_Trained_AEnet\n",
    "k = 10\n",
    "AEnet = get_Trained_AEnet(dataset_from_matrix_df=dataset_from_matrix_df, z_dim=k, num_of_epochs=3, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "k = 10\n",
    "custom_DS_LatentTensor_AE = loadAndPreProcess.STDL_Dataset_KValuesPerImg_LatentTensor_AutoEncoder(imageFolder=imageFolder_train, \n",
    "                                                                           matrix_dataframe=matrix_dataframe_train, \n",
    "                                                                           features_dataframe=features_dataframe_train, \n",
    "                                                                           barcodes_dataframe=barcodes_dataframe_train, \n",
    "                                                                           AEnet=AEnet,\n",
    "                                                                           column_mapping=column_mapping_train,\n",
    "                                                                           num_of_dims_k=k,\n",
    "                                                                           device=device)\n",
    "custom_DS_LatentTensor_AE_augmented = loadAndPreProcess.STDL_Dataset_KValuesPerImg_LatentTensor_AutoEncoder(imageFolder=augmentedImageFolder_train, \n",
    "                                                                           matrix_dataframe=matrix_dataframe_train, \n",
    "                                                                           features_dataframe=features_dataframe_train, \n",
    "                                                                           barcodes_dataframe=barcodes_dataframe_train, \n",
    "                                                                           AEnet=AEnet,                                                                                                            \n",
    "                                                                           column_mapping=column_mapping_train,\n",
    "                                                                           num_of_dims_k=k,\n",
    "                                                                           device=device)\n",
    "custom_DS_LatentTensor_AE_test = loadAndPreProcess.STDL_Dataset_KValuesPerImg_LatentTensor_AutoEncoder(imageFolder=imageFolder_test, \n",
    "                                                                           matrix_dataframe=matrix_dataframe_test, \n",
    "                                                                           features_dataframe=features_dataframe_test, \n",
    "                                                                           barcodes_dataframe=barcodes_dataframe_test, \n",
    "                                                                           AEnet=AEnet,                                                                                                       \n",
    "                                                                           column_mapping=column_mapping_test,\n",
    "                                                                           num_of_dims_k=k,\n",
    "                                                                           device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9: prepare for the next phases in which the experiments are executed\n",
    "\n",
    "> import `executionModule` which contains the experiments, training methods, and testing methods\n",
    "\n",
    "> create `hyperparameters` dictionary which will contain all of the hyper-parameters for our experiments (note - user can change these later)\n",
    "\n",
    "> create `model_list` that will hold all the names for the models that will be used (only 3 models for now, as can be seen below). the models are:\n",
    "\n",
    ">> `BasicConvNet` model\n",
    "\n",
    ">> `DensetNet121` model\n",
    "\n",
    ">> `Inception_V3` model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b> change the hyper-parameters below with caution if needed !\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import executionModule\n",
    "\n",
    "# define hyperparameters for the experiments\n",
    "hyperparameters = dict()\n",
    "hyperparameters['batch_size'] = 25\n",
    "hyperparameters['max_alowed_number_of_batches'] = 99999\n",
    "hyperparameters['precent_of_dataset_allocated_for_training'] = 0.8\n",
    "hyperparameters['learning_rate'] = 1e-4\n",
    "hyperparameters['momentum'] = 0.9\n",
    "hyperparameters['num_of_epochs'] = 3\n",
    "\n",
    "# define hyperparameters for BsicConvNet\n",
    "hyperparameters['channels'] = [32] \n",
    "hyperparameters['num_of_convolution_layers'] = len(hyperparameters['channels'])\n",
    "hyperparameters['hidden_dims'] = [100]\n",
    "hyperparameters['num_of_hidden_layers'] = len(hyperparameters['hidden_dims'])\n",
    "hyperparameters['pool_every'] = 99999\n",
    "\n",
    "# list of all models used\n",
    "model_list = []\n",
    "model_list.append('BasicConvNet')\n",
    "model_list.append('DensetNet121')\n",
    "# model_list.append('Inception_V3')  #TODO: still need to sort out input image size... !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Note:</b> inception_v3 model isnt sorted out yet... gives\n",
    "    RuntimeError: Calculated padded input size per channel: (2 x 2). Kernel size: (5 x 5). Kernel size can't be greater than actual input size\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> creating an assisting method for our testing that will time each experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_loop(ds_train, ds_test, phase_name):\n",
    "    for model_name in model_list:\n",
    "        print(f'\\nstarting experiment **{model_name}**\\n')\n",
    "        %time executionModule.runExperiment(ds_train=ds_train, ds_test=ds_test, hyperparams=hyperparameters, device=device, model_name=model_name, dataset_name=phase_name)\n",
    "        print(f'\\nfinished experiment {model_name}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Single Gene Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_loop(ds_train=custom_DS_SingleValuePerImg ,ds_test=custom_DS_SingleValuePerImg_test ,phase_name='single_gene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_loop(ds_train=custom_DS_SingleValuePerImg_augmented ,ds_test=custom_DS_SingleValuePerImg_test ,phase_name='single_gene_augmented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: K genes prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_loop(ds_train=custom_DS_KGenesWithHighestVariance ,ds_test=custom_DS_KGenesWithHighestVariance_test ,phase_name='k_genes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_loop(ds_train=custom_DS_KGenesWithHighestVariance_augmented ,ds_test=custom_DS_KGenesWithHighestVariance_test ,phase_name='k_genes_augmented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: All genes prediction - using dimensionality reduction techniques\n",
    "\n",
    "### 4.1: Prediction using dimensionality reduction technique NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_loop(ds_train=custom_DS_LatentTensor_NMF ,ds_test=custom_DS_LatentTensor_NMF_test ,phase_name='NMF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_loop(ds_train=custom_DS_LatentTensor_NMF_augmented ,ds_test=custom_DS_LatentTensor_NMF_test ,phase_name='NMF_augmented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Prediction using dimensionality reduction technique AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_loop(ds_train=custom_DS_LatentTensor_AE ,ds_test=custom_DS_LatentTensor_AE_test ,phase_name='AE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_loop(ds_train=custom_DS_LatentTensor_AE_augmented ,ds_test=custom_DS_LatentTensor_AE_test ,phase_name='AE_augmented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Note:</b> below this - everything is a testing block\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
